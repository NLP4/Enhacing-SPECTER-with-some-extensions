{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NLP4/Investigating-the-Versatility-of-SPECTER/blob/main/Temporal%20weights%20classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StDe-NXo9Yo9"
      },
      "outputs": [],
      "source": [
        "#connection to Kaggle API\n",
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_6Qv1hh9a2P"
      },
      "outputs": [],
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFEG5o-z9n5j"
      },
      "outputs": [],
      "source": [
        "#download the arxiv dataset\n",
        "!kaggle datasets download -d Cornell-University/arxiv\n",
        "!chmod 600 ~/arxiv-metadata-oai-snapshot.json\n",
        "!unzip arxiv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apHRN4be9tdm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf2ZTI4I93yj"
      },
      "outputs": [],
      "source": [
        "#Using `yield` to load the JSON file in a loop to prevent Python memory issues if JSON is loaded directly\n",
        "data_file = '/content/arxiv-metadata-oai-snapshot.json'\n",
        "\n",
        "def get_metadata():\n",
        "    with open(data_file, 'r') as f:\n",
        "        for line in f:\n",
        "            yield line\n",
        "\n",
        "metadata = get_metadata()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_Ng7LUr-Cye"
      },
      "outputs": [],
      "source": [
        "#extracting the year from the 'journal-ref' column and creating separate list for each column\n",
        "metadata = get_metadata()\n",
        "ids = []\n",
        "titles = []\n",
        "abstracts = []\n",
        "categories = []\n",
        "years = []\n",
        "for paper in metadata:\n",
        "    metaDict = json.loads(paper)\n",
        "    try:\n",
        "        try:\n",
        "            year = int(metaDict['journal-ref'][-4:])    ### Example Format: \"Phys.Rev.D76:013009,2007\"\n",
        "        except:\n",
        "            year = int(metaDict['journal-ref'][-5:-1])    ### Example Format: \"Phys.Rev.D76:013009,(2007)\"\n",
        "        \n",
        "        ids.append(metaDict['id'])\n",
        "        titles.append(metaDict['title'])\n",
        "        abstracts.append(metaDict['abstract'])\n",
        "        categories.append(metaDict['categories'])\n",
        "        years.append(year)\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQw6yDgC-YiP"
      },
      "outputs": [],
      "source": [
        "#creating a data frame from the lists with the corresponding columns and the year included\n",
        "df = pd.DataFrame({'id' : ids,'Title' : titles,'Abstract' : abstracts, 'Year' : years, 'Categories' : categories})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkmbzKaw1oQo"
      },
      "outputs": [],
      "source": [
        "min_year = df['Year'].min()\n",
        "print(min_year)\n",
        "max_year = df['Year'].max()\n",
        "print(max_year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD9Get62_Nf2"
      },
      "outputs": [],
      "source": [
        "#selecting only papers from year 1990 to year 2022 \n",
        "df1 = df[(df['Year'] > 1990) & (df['Year'] < 2022)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZohdrIs_S3h"
      },
      "outputs": [],
      "source": [
        "#renaming categories with more representable names\n",
        "maskM = df1['Categories'].str.contains('math')\n",
        "df1.loc[maskM, 'Categories'] = 'Maths'\n",
        "phys = ['ph', 'mat', 'qc', 'hep', 'nlin', 'nucl', 'physics', 'quant']\n",
        "pattern = '|'.join(phys)\n",
        "maskP = df1['Categories'].str.contains(pattern)\n",
        "df1.loc[maskP, 'Categories'] = 'Physics'\n",
        "maskB = df1['Categories'].str.contains('bio')\n",
        "df1.loc[maskB, 'Categories'] = 'Biology'\n",
        "maskF = df1['Categories'].str.contains('fin')\n",
        "df1.loc[maskF, 'Categories'] = 'Finance'\n",
        "maskS = df1['Categories'].str.contains('stat')\n",
        "df1.loc[maskS, 'Categories'] = 'Statistics'\n",
        "maskCS = df1['Categories'].str.contains('cs')\n",
        "df1.loc[maskCS, 'Categories'] = 'Computer Science'\n",
        "maskE = df1['Categories'].str.contains('econ')\n",
        "df1.loc[maskE, 'Categories'] = 'Economics'\n",
        "maskES = df1['Categories'].str.contains('eess')\n",
        "df1.loc[maskES, 'Categories'] = 'Electrical Engineering and Systems Science'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_sizes = df1.groupby('Categories').size()\n",
        "group_sizes"
      ],
      "metadata": {
        "id": "DmGN3uzNzOIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeVB_-9i_ZYk"
      },
      "outputs": [],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2TkFUKOAB8C"
      },
      "outputs": [],
      "source": [
        "#new dataset with equal number of papers from each category\n",
        "n_samples = 1000\n",
        "grouped = df1.groupby('Categories', group_keys=False).apply(lambda x: x.sample(n=min(n_samples, len(x)), random_state=42))\n",
        "df2 = grouped.sample(frac=1, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACgupTnS_e_I"
      },
      "outputs": [],
      "source": [
        "#randomly sampling papers\n",
        "df3 = df2.sample(n=3000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3"
      ],
      "metadata": {
        "id": "oqTTfkmzta93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#representation of the categories\n",
        "df4 = df3\n",
        "df4.drop('id', inplace=True, axis=1)\n",
        "groups = df4.groupby('Categories').first()\n",
        "groups"
      ],
      "metadata": {
        "id": "mWaTMEvQyj4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gExYz8U1_vAT"
      },
      "outputs": [],
      "source": [
        "group_sizes = df3.groupby('Categories').size()\n",
        "group_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfTgzfQWf6JK"
      },
      "outputs": [],
      "source": [
        "min_year = df3['Year'].min()\n",
        "print(min_year)\n",
        "max_year = df3['Year'].max()\n",
        "print(max_year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyaLfw9C_19q"
      },
      "outputs": [],
      "source": [
        "#distribution of number of papers in different years\n",
        "import matplotlib.pyplot as plt\n",
        "year_counts = df3['Year'].value_counts()\n",
        "\n",
        "year_counts.plot(kind='bar')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of papers in different years')\n",
        "plt.ylim(0, 400)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNbrtveSAS1h"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets==1.2.1\n",
        "!pip install transformers\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c48m71TPyzsT"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#preprocessing function that removes removes URLs and email addresses, removes non-alphanumeric characters and converts the text to lowercase, tokenizes the text into words, removes stop words, and re-joins the words into a single string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess(text):\n",
        "    # Remove any URLs or email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove any non-alphanumeric characters and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove any stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if not word in stop_words]\n",
        "\n",
        "    # Rejoin the words into a single string\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxbjoatGAjck"
      },
      "outputs": [],
      "source": [
        "#using SPECTER to compute the embeddings\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "# Load the SPECTER model and tokenizer\n",
        "model_name = \"allenai/specter\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Compute the embeddings in batches\n",
        "batch_size = 12\n",
        "num_batches = (len(df3) + batch_size - 1) // batch_size\n",
        "\n",
        "embeddings = []\n",
        "for i in range(num_batches):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min((i + 1) * batch_size, len(df3))\n",
        "    batch = df3.iloc[start_idx:end_idx]\n",
        "    inputs = list(batch.apply(lambda row: f\"{row['Title']} {tokenizer.sep_token} {row['Abstract']}\", axis=1))\n",
        "\n",
        "    # Tokenize the inputs and pad the sequences\n",
        "    encoded_inputs = tokenizer(inputs, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "    padded_inputs = {k: v.to(model.device) for k, v in encoded_inputs.items()}\n",
        "\n",
        "    # Compute the embeddings for the batch\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**padded_inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "\n",
        "    embeddings.append(batch_embeddings)\n",
        "\n",
        "# Concatenate the embeddings for all batches\n",
        "embeddings = np.concatenate(embeddings, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNjrrrgDqXQ4"
      },
      "source": [
        "**No Weights In The Classification Process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6S7Bn38VqSvG"
      },
      "outputs": [],
      "source": [
        "#splitting the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trainNW, X_testNW, y_trainNW, y_testNW = train_test_split(embeddings, df3['Categories'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwneD4s0qSvH"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clfNW = LinearSVC()\n",
        "clfNW.fit(X_trainNW, y_trainNW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3l8_v9aqSvH"
      },
      "outputs": [],
      "source": [
        "y_predNW = clfNW.predict(X_testNW)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_testNW, y_predNW))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2EC0FWNAnyj"
      },
      "source": [
        "**Harmonic Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTLEhvX2Aehq"
      },
      "outputs": [],
      "source": [
        "#weights with harmonic function\n",
        "weightsH = np.zeros(len(df3))\n",
        "\n",
        "max_year = df3['Year'].max()\n",
        "min_year = df3['Year'].min()\n",
        "center_year = (min_year + max_year) / 2\n",
        "for i, year in enumerate(df3['Year']):\n",
        "    weightsH[i] = 1 / (1 + abs(year - center_year))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYkFboivAs7W"
      },
      "outputs": [],
      "source": [
        "#splitting the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trainH, X_testH, y_trainH, y_testH, weights_trainH, weights_testH = train_test_split(embeddings, df3['Categories'], weightsH, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDRSOiQ-A0oc"
      },
      "outputs": [],
      "source": [
        "#the weights array had to be reshaped to be able to multiply it with the training data\n",
        "weights_trainH = weights_trainH.reshape((-1, 1))\n",
        "X_train_weightedH = X_trainH*weights_trainH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Ac-CTUA6fn"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train_weightedH, y_trainH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htUr6uODA83j"
      },
      "outputs": [],
      "source": [
        "X_test_weightedH = X_testH*weights_testH.reshape((-1, 1))\n",
        "y_predH = clf.predict(X_test_weightedH)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_testH, y_predH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZUFhVNyDUK3"
      },
      "source": [
        "**Linear Weights** \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwYdx-HdAPRT"
      },
      "outputs": [],
      "source": [
        "#weights with linear function\n",
        "weightsL = np.zeros(len(df3))\n",
        "\n",
        "max_year = df3['Year'].max()\n",
        "min_year = df3['Year'].min()\n",
        "for i, year in enumerate(df3['Year']):\n",
        "    weightsL[i] = (max_year - year + 1) / (max_year - min_year + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SZdnjYWPtH7"
      },
      "outputs": [],
      "source": [
        "#splitting the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trainL, X_testL, y_trainL, y_testL, weights_trainL, weights_testL = train_test_split(embeddings, df3['Categories'], weightsL, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZOe4OlctQRo"
      },
      "outputs": [],
      "source": [
        "#the weights array had to be reshaped to be able to multiply it with the training data\n",
        "weights_trainL = weights_trainL.reshape((-1, 1))\n",
        "X_train_weightedL = X_trainL*weights_trainL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QdSB0xEDDdE"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train_weightedL, y_trainL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30zFxWgoDJFn"
      },
      "outputs": [],
      "source": [
        "X_test_weightedL = X_testL*weights_testL.reshape((-1, 1))\n",
        "y_pred = clf.predict(X_test_weightedL)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_testL, y_predL))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DsB6i9QDLwy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3E_lEE26nf1"
      },
      "source": [
        "**Exponential Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTYiC7iOXFb3"
      },
      "outputs": [],
      "source": [
        "decay_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOrnT2jhXF11"
      },
      "outputs": [],
      "source": [
        "#weights with exponential function\n",
        "current_year = df3['Year'].max()\n",
        "weightsE = np.zeros(len(df3))\n",
        "for i, year in enumerate(df3['Year']):\n",
        "    weightsE[i] = np.exp(-decay_rate * (current_year - year))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4IZRxRWXKX-"
      },
      "outputs": [],
      "source": [
        "#splitting the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trainE, X_testE, y_trainE, y_testE, weights_trainE, weights_testE = train_test_split(embeddings, df3['Categories'], weightsE, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQP8Bchzt6vk"
      },
      "outputs": [],
      "source": [
        "#the weights array had to be reshaped to be able to multiply it with the training data\n",
        "weights_trainE = weights_trainE.reshape((-1, 1))\n",
        "X_train_weightedE = X_trainE*weights_trainE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OURCDZwYXRug"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train_weightedE, y_trainE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxc7fcQ1XTUu"
      },
      "outputs": [],
      "source": [
        "X_test_weightedE = X_testE*weights_testE.reshape((-1, 1))\n",
        "y_predE = clf.predict(X_test_weightedE)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_testE, y_predE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEFg1Ux1XVTn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCeGI6nDsaIdh5r0XY60Dm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}