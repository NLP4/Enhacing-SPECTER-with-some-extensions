# -*- coding: utf-8 -*-
"""Specter + classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CAZQ9FaTqXiE3Fsb-hTFgaX4iNwSxKVo

This notebook represent the baseline for testing the classification task using the CORD19 dataset and generating the SPECTER embeddings using title and abstract
"""

!git clone https://github.com/allenai/specter.git

!wget https://ai2-s2-research-public.s3-us-west-2.amazonaws.com/specter/archive.tar.gz

!pip install -U -q kaggle
!mkdir -p ~/.kaggle
!echo '{"username":"oumaimaregragui","key":"98f2ff74d27102ab393ebb934d047f83"}' > ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d dillonpulliam/cord19cleaneddata

!unzip cord19cleaneddata.zip

import re
import numpy as np
import pandas as pd
import pickle
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer

metadata_path = '/content/covidDataCleaned.csv'
meta_df = pd.read_csv(metadata_path, dtype={'doi': str})

meta_df = meta_df[['paper_id','abstract','body_text','title','authors','journal','url']].reset_index(drop=True)

def get_label(row):
    abstract = str(row['abstract']).lower() # convert abstract to string and lowercase
    if "covid-19" in abstract or "coronavirus" in abstract or "sars-cov-2" in abstract:
        return "relevant"
    else:
        return "irrelevant"
meta_df['label'] = meta_df.apply(get_label, axis=1)

meta_df.head(5)

import pandas as pd
import re
import numpy as np
import spacy

# Load the spaCy model
nlp = spacy.load("en_core_web_sm")

# Define a regular expression to match dates
date_regex = re.compile(r"\d{4}")

# Create a new column for the date
meta_df["date"] = ""

# Process each abstract and extract the date
for i, abstract in enumerate(meta_df["abstract"]):
    if isinstance(abstract, float) and np.isnan(abstract):
        abstract = ""
    doc = nlp(abstract)
    for token in doc:
        if token.pos_ == "NUM" and date_regex.match(token.text):
            date = token.text
            break
    else:
        date = ""
    
    # Store the extracted date in the date column
    meta_df.at[i, "date"] = date if isinstance(date, str) else str(date)

meta_df = meta_df[meta_df['date'].notnull() & (meta_df['date'] > '2018') & (meta_df['date'] < '2024')]

!pip install transformers

import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel

# Load the metadata
meta_df = meta_df[['paper_id','abstract','body_text','title','authors','journal','url']].reset_index(drop=True)

# Load the SPECTER model and tokenizer
model_name = "allenai/specter"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Compute the embeddings in batches
batch_size = 12
num_batches = (len(meta_df) + batch_size - 1) // batch_size

embeddings = []
for i in range(num_batches):
    start_idx = i * batch_size
    end_idx = min((i + 1) * batch_size, len(meta_df))
    batch = meta_df.iloc[start_idx:end_idx]
    inputs = list(batch.apply(lambda row: f"{row['title']} {row['abstract']}", axis=1))

    # Tokenize the inputs and pad the sequences
    encoded_inputs = tokenizer(inputs, padding=True, truncation=True, max_length=512, return_tensors='pt')
    padded_inputs = {k: v.to(model.device) for k, v in encoded_inputs.items()}

    # Compute the embeddings for the batch
    with torch.no_grad():
        outputs = model(**padded_inputs)
        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

    embeddings.append(batch_embeddings)

# Concatenate the embeddings for all batches
embeddings = np.concatenate(embeddings, axis=0)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(embeddings, meta_df['label'], test_size=0.2, random_state=42)

# Train a logistic regression classifier
clf = LogisticRegression(random_state=42, max_iter=1000)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the performance of the classifier
print(classification_report(y_test, y_pred))
